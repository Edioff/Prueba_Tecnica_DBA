# ğŸ“Œ Prueba TÃ©cnica de DBA

Este repositorio contiene la soluciÃ³n a la prueba tÃ©cnica de DBA. Se han implementado consultas SQL, triggers en PostgreSQL y pipelines ETL en PySpark para la detecciÃ³n de patrones sospechosos en apuestas.

ğŸ“„ **DocumentaciÃ³n completa en Notion:** [ğŸ”— Ver Documento](https://mangrove-red-378.notion.site/Prueba-T-cnica-16f48925011e80b0a626f8089a1454e3)

---

## ğŸ“ Estructura del Proyecto

El proyecto estÃ¡ organizado en las siguientes carpetas:

Prueba_tecnica_dba/ â”‚â”€â”€ python/ # Scripts ETL en PySpark â”‚ â”œâ”€â”€ pipeline_2_1.py # Detecta usuarios con mÃ¡s de 50 apuestas en 10 minutos â”‚ â”œâ”€â”€ pipeline_2_2.py # Detecta mÃ¡s de 20 apuestas idÃ©nticas en 5 minutos â”‚ â”œâ”€â”€ simulacion_datos.py # GeneraciÃ³n de datos de prueba â”‚ â”‚â”€â”€ recursos/ # Dependencias necesarias â”‚ â”œâ”€â”€ postgresql-42.7.4.jar # Driver JDBC para PostgreSQL â”‚ â”‚â”€â”€ sql/ # Scripts SQL organizados por categorÃ­a â”‚ â”œâ”€â”€ esquema/ # DefiniciÃ³n de tablas y relaciones â”‚ â”œâ”€â”€ indices/ # CreaciÃ³n de Ã­ndices optimizados â”‚ â”œâ”€â”€ patrones/ # Consultas SQL para detecciÃ³n de patrones â”‚ â”œâ”€â”€ triggers/ # ImplementaciÃ³n de triggers en PostgreSQL

yaml
Copiar cÃ³digo

---

## ğŸš€ Requisitos Previos

Antes de ejecutar el proyecto, asegÃºrate de tener instalado lo siguiente:

- **PostgreSQL** (mÃ­nimo versiÃ³n 12)
- **Python 3.8+**
- **Apache Spark** con PySpark
- **Conector JDBC para PostgreSQL** (`postgresql-42.7.4.jar`)

Para instalar dependencias en Python:

```sh
pip install pyspark psycopg2 pandas
ğŸ”§ ConfiguraciÃ³n
Antes de ejecutar los scripts, modifica las credenciales de conexiÃ³n en los archivos Python para que coincidan con tu base de datos:

python
Copiar cÃ³digo
DB_CONFIG = {
    "dbname": "db",
    "user": "postgres",
    "password": "tu_contraseÃ±a",
    "host": "localhost",
    "port": "5432"
}
âš ï¸ IMPORTANTE:

Es necesario modificar el nombre de la base de datos, usuario, contraseÃ±a, host y puerto en la configuraciÃ³n de psycopg2 y JDBC en los archivos pipeline_2_1.py, pipeline_2_2.py y simulacion_datos.py.
TambiÃ©n es posible que debas ajustar los directorios donde estÃ¡n almacenados los archivos de recursos (recursos/postgresql-42.7.4.jar).
ğŸ› ï¸ Instrucciones de Uso
1ï¸âƒ£ Configurar PostgreSQL
Ejecutar los scripts en sql/esquema/ para crear las tablas.
2ï¸âƒ£ Crear Ãndices
Ejecutar los scripts en sql/indices/ para optimizar el rendimiento.
3ï¸âƒ£ Configurar los Triggers
Ejecutar los archivos en sql/triggers/ para activar la detecciÃ³n automÃ¡tica de patrones.
4ï¸âƒ£ Ejecutar los Pipelines ETL en PySpark
Para analizar las apuestas y detectar patrones sospechosos:

sh
Copiar cÃ³digo
python python/pipeline_2_1.py  # Detecta usuarios con mÃ¡s de 50 apuestas en 10 minutos
python python/pipeline_2_2.py  # Detecta mÃ¡s de 20 apuestas idÃ©nticas en 5 minutos
5ï¸âƒ£ Consultar Patrones Detectados
Se pueden visualizar los patrones sospechosos ejecutando las queries en sql/patrones/.
ğŸ“Š Resultados y OptimizaciÃ³n
Se utilizaron Ã­ndices y EXPLAIN ANALYZE para optimizar las consultas. Se analizaron los triggers de PostgreSQL, evaluando su impacto en el rendimiento:

Tiempo de ejecuciÃ³n de los triggers en Apuestas:
PatrÃ³n 2.1 (Usuarios con +50 apuestas en 10 minutos) â†’ ~1.985 ms
PatrÃ³n 2.2 (MÃ¡s de 20 apuestas idÃ©nticas en 5 minutos) â†’ ~0.090 ms
ğŸ“Š MÃ¡s detalles en el anÃ¡lisis de rendimiento: ğŸ”— Ver Documento en Notion

ğŸ’¡ Mejoras Futuras
ğŸ”¹ Optimizar las consultas para manejar grandes volÃºmenes de datos.
ğŸ”¹ Implementar particionamiento en la base de datos.
ğŸ”¹ Evaluar una arquitectura basada en Apache Kafka + Spark Streaming para detecciÃ³n en tiempo real.
